from keras import Sequential
from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
import tensorflow as tf
import set_onehot_encoding as onehot
import os
import keras

total_features = 545333  # total unique features
path = "defensive_distillation/"

if not os.path.exists(path):  # check if path exists
    os.mkdir(path)
print("Creating data-labels...")
onehot.create_list_of_apps()  # function from set_one_encoding.py


def create_training_input():
    if os.path.isfile("training_set_1500.txt") is False:
        set_size = 1500
        malware_ratio = 0.3
        print("Creating data-labels...")
        print("Generating TESTING set...")
        training_set = onehot.generate_set(set_size, malware_ratio)  # generate random testing set
        with open("training_set_1500.txt", "w") as file:
            for item in training_set:
                file.write(str(item) + "\n")
    training_set = []  # the list of testing set
    with open("training_set_1500.txt", "r") as file:  # read testing set file and append applications to list
        for line in file:
            line.strip()
            line = line[:-1]
            training_set.append(line)
    print("Generating TESTING input...")
    data, labels = onehot.generate_input(training_set, total_features)  # perform one-hot encoding
    return data, labels


def create_testing_input():
    if os.path.isfile("testing_set_1500.txt") is False:
        set_size = 1500
        malware_ratio = 0.3
        print("Creating data-labels...")
        print("Generating TESTING set...")
        testing_set = onehot.generate_set(set_size, malware_ratio)  # generate random testing set
        with open("testing_set_1500.txt", "w") as file:
            for item in testing_set:
                file.write(str(item) + "\n")
    testing_set = []  # the list of testing set
    with open("testing_set_1500.txt", "r") as file:  # read testing set file and append applications to list
        for line in file:
            line.strip()
            line = line[:-1]
            testing_set.append(line)
    print("Generating TESTING input...")
    test_data, test_labels = onehot.generate_input(testing_set, total_features)  # perform one-hot encoding
    return test_data, test_labels


def train(train_data, train_labels, test_data, test_labels, file_name,
          epochs=4, batch_size=150, train_temp=1, init=None, callbacks=False):
    # neural net parameters
    units = [200, 200]
    activation_function = "relu"
    kernel = "glorot_uniform"
    bias = "zeros"
    dropout = 0.2
    learn_rate = 0.001

    model = Sequential()  # neural net init
    model.add(Dense(units=units[0], activation=activation_function, input_dim=total_features, kernel_initializer=kernel,
                    bias_initializer=bias))
    model.add(Dropout(dropout))  # add dropout rate

    for hidden_layer_units in units[1:]:  # add hidden layers defined units in train_models.py
        model.add(Dense(units=hidden_layer_units, activation=activation_function, kernel_initializer=kernel,
                        bias_initializer=bias))
        model.add(Dropout(dropout))

    model.add(Dense(2))  # output layer, with  with two neurons and without activation function

    if init is not None:
        model.load_weights(init)

    def fn(correct, predicted):
        return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits=(predicted / train_temp))

    # loss the fn method defined above, Adam optimizer
    model.compile(loss=fn,
                  optimizer=Adam(lr=learn_rate),
                  metrics=["accuracy"])

    if callbacks:
        log_dir = path + "log/dir/DNN"
        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)
        early_stopping_callback = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=2)
        model_checkpoint_callback = ModelCheckpoint(file_name, monitor='val_accuracy',
                                                    mode='max',
                                                    verbose=2, save_best_only=True)
        model.fit(train_data, train_labels,
                  epochs=epochs,
                  batch_size=batch_size,
                  validation_data=(test_data, test_labels),
                  callbacks=[tensorboard_callback, early_stopping_callback, model_checkpoint_callback],
                  verbose=2)
    else:
        model.fit(train_data, train_labels,
                  epochs=epochs,
                  batch_size=batch_size,
                  validation_data=(test_data, test_labels),
                  verbose=2)

        if file_name is not None:
            model.save(file_name)

    return model


def train_distillation(features, labels, file_name, epochs=4, batch_size=150, train_temp=1):
    """
    :param features: the train data
    :param labels: the train labels
    :param file_name: the file to save teacher and student
    :param epochs: number of epochs
    :param batch_size: batch size
    :param train_temp: temperature
    :return:
    """
    if not os.path.exists(file_name + "_init"):
        # train for one epoch to get a starting point
        train(features, labels, test_data, test_labels, file_name + "_init", 1, batch_size)

    # train the teacher at the given temperature
    print("Temperature:", train_temp)
    teacher = train(features, labels, test_data, test_labels, file_name + "_teacher", epochs, batch_size,
                    train_temp, init=file_name + "_init")

    predicted = teacher.predict(features)  # evaluate the labels at the given temperature
    print(predicted)

    with tf.compat.v1.Session() as sess:
        y = sess.run(tf.nn.softmax(predicted / train_temp))
        print(y)
        train_labels = y

    # train the student at temperature t
    student = train(features, train_labels, test_data, test_labels, file_name, epochs, batch_size,
                    train_temp, init=file_name + "_init")
    # predict at temperature 1
    predicted = student.predict(features)
    # print(predicted)


data, labels = create_training_input()  # init train data-labels
test_data, test_labels = create_testing_input()  # init test data-labels
# we use categorical_cross_entropy and thus, an encode to one hot labels is required
labels = keras.utils.to_categorical(labels, 2)
test_labels = keras.utils.to_categorical(test_labels, 2)

# first train with original temperature (= 1)
train(data, labels, test_data, test_labels, path + "original", epochs=30, callbacks=True)
# train teacher and student networks with a predefined temperature
train_distillation(data, labels, path + "distilled-100", epochs=7, batch_size=150, train_temp=120)
