import set_onehot_encoding as onehot
import models
import neural_network as NN


def create_sets():
    training_set = []  # the list of training set
    testing_set = []  # the list of testing set

    with open("training_set_1500.txt", "r") as file:  # read training set file and append applications to list
        for line in file:
            line.strip()  # remove whitespace
            line = line[:-1]  # remove \n
            training_set.append(line)  # add item to list
    with open("testing_set_1500.txt", "r") as file:  # read testing set file and append applications to list
        for line in file:
            line.strip()
            line = line[:-1]
            testing_set.append(line)
    print("Generating TRAINING input...")
    data, labels = onehot.generate_input(training_set, total_features)  # perform one-hot encoding
    print("Generating TESTING input...")
    test_data, test_labels = onehot.generate_input(testing_set, total_features)  # perform one-hot encoding
    return data, labels, test_data, test_labels


def train_models():
    data, labels, test_data, test_labels = create_sets()
    """
    for classic machine learning model, e.g., Naive Bayes, Decision Tree etc, first we fit the classifier and
    then we evaluate on the test. The best hyperparameters found from the grid search procedure are defined
    in the models.py helper script.
    """
    #model = GNB.train_gaussian_naive_bayes_classifier(data, labels, save=True)  # train Naive Bayes
    #GNB.evaluate_gaussian_naive_bayes_classifier(model, test_data, test_labels)  # test performance

    #model = MNB.train_multi_naive_bayes_classifier(data, labels, save=True)
    #MNB.evaluate_multi_naive_bayes_classifier(model, test_data, test_labels)

    #model = CNB.train_complement_naive_bayes_classifier(data, labels, save=True)
    #CNB.evaluate_complement_naive_bayes_classifier(model, test_data, test_labels)

    #model = BNB.train_bernoulli_naive_bayes_classifier(data, labels, save=True)
    #BNB.evaluate_bernoulli_naive_bayes_classifier(model, test_data, test_labels)

    #model = DT.train_decision_tree_classifier(data, labels, save=True)
    #DT.evaluate_decision_tree_classifier(model, test_data, test_labels)

    #model = RF.train_random_forest_classifier(data, labels, save=True)
    #RF.evaluate_random_forest_classifier(model, test_data, test_labels)

    #model = KNN.train_knn_classifier(data, labels, save=True)
    #KNN.evaluate_knn_classifier(model, test_data, test_labels)

    #model = LR.train_logistic_regression_classifier(data, labels, save=True)
    #LR.evaluate_logistic_regression_classifier(model, test_data, test_labels)

    #model = SVM.train_svm_classifier(data, labels, save=True)
    #SVM.evaluate_svm_classifier(model, test_data, test_labels)

    # init the neural net
    model = NN.generate_neural_network(total_features, units, dropout, learn_rate, kernel_initializer,
    #                                  bias_initializer, activation_function)
    """
    train the neural network with the given model, epochs, batch size, train data-labels.
    Specify verbosity level, validation data, callbacks and plots (if needed).
    Default parameters:
    verbose=0, validation=False, val_data=None, val_labels=None, callbacks=False, plot_history=False
    example:
    NN.train_neural_network(model, epochs, batch_size, data, labels, verbose=0,
                            validation=True, val_data=test_data, val_labels=test_labels,
                            callbacks=True, plot_history=True)
    This is the main training stage and thus we want to save the best models at the 'right time'. This is done
    setting the callback to True. Keras will seek for the minimum validation loss and it saves the model with
    the highest validation accuracy.
    """
    NN.train_neural_network(model, epochs, batch_size, data, labels, verbose=2,
                            validation=True, val_data=test_data, val_labels=test_labels,
                            callbacks=True)


if __name__ == "__main__":
    total_features = 545333  # total unique features
    set_size = 1500  # set site that will be used to create random training set
    testing_set_size = 1500  # set site that will be used to create random test set
    malware_ratio = 0.3  # malware ratio in the set size

    print("Creating data-labels...")
    onehot.create_list_of_apps()  # function from set_one_encoding.py

    # initialize sklearn models (classic machine learning)
    GNB = models.GaussianNaiveBayes()
    MNB = models.MultinomialNaiveBayes()
    CNB = models.ComplementNaiveBayes()
    BNB = models.BernoulliNaiveBayes()
    DT = models.DecisionTree()
    RF = models.RandomForest()
    KNN = models.KNearestNeighbors()
    LR = models.LogRegression()
    SVM = models.SupportVectorMachine()

    # neural net parameters
    units = [200, 200]
    dropout = 0.2  # dropout rate to avoid over fitting (Note that dropout alone is not efficient)
    epochs = 4  # set maximum epochs to 20. If callbacks are specified Keras will automatically stop the procedure
    batch_size = 150  # we found that the batch size of 150 fits better in our task
    learn_rate = 0.001  # specify the learning rate according to the optimizer used
    kernel_initializer = 'glorot_uniform'  # weight initialization
    bias_initializer = 'zeros'  # bias initialization
    activation_function = 'relu'  # activation function

    train_models()
