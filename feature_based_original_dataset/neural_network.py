from sklearn.metrics import confusion_matrix
import timeit
from keras import Sequential
from keras.layers import Dense, Dropout
import numpy as np
import matplotlib.pyplot as plt
from keras.callbacks import TensorBoard
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam

average_FNR = 0
average_FPR = 0
average_accuracy = 0


def generate_neural_network(total_features, units, dropout, learn_rate, kernel, bias, activation_function):
    """
    :param total_features: the total features (input_dim) we used to train our network
    :param units: neurons in the hidden layers
    :param dropout: the dropout rate
    :param learn_rate: learning rate
    :param kernel: (kernel_initializer) weights initialization
    :param bias: (bias_initializer) bias init initialization
    :param activation_function: activation function
    :return:
    """
    model = Sequential()  # neural net init
    """
    add input layer dimension with 545333 features
    hidden layers with the defined units, dropout rate, weight and biases initialization,
    relu activation function and softmax in output layer
    """
    model.add(Dense(units=units[0], activation=activation_function, input_dim=total_features, kernel_initializer=kernel,
                    bias_initializer=bias))
    model.add(Dropout(dropout))  # add dropout rate

    for hidden_layer_units in units[1:]:  # add hidden layers defined units in train_models.py
        model.add(Dense(units=hidden_layer_units, activation=activation_function, kernel_initializer=kernel,
                        bias_initializer=bias))
        model.add(Dropout(dropout))

    model.add(Dense(2, activation="softmax"))  # output layer, with softmax activation function and 2 neurons

    # loss: sparse categorical cross entropy, Optimizer: Adam
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer=Adam(lr=learn_rate),
                  metrics=["accuracy"])

    """
    information about the NN, such as the number of layers, the output shape, 
    the number of weights in each layer and the total weights.
    """
    #model.summary()

    # plot of the neural network graph
    #plot_model(model, to_file="figures/DNN_model_plot.png", show_shapes=True, show_layer_names=True)

    return model


def train_neural_network(model, epochs, batch_size, features, labels, verbose=0,
                         validation=False, val_data=None, val_labels=None,
                         callbacks=False, plot_history=False, path="logs/fit/", model_name="DNN_200_200"):
    """
    :param modelh5: neural network model from generate_neural_network()
    :param epochs: number of epochs
    :param batch_size: batch size
    :param features: training data
    :param labels: training labels
    :param verbose: verbosity level
    :param validation: if True validate data
    :param val_data: validation data
    :param val_labels: validation labels
    :param callbacks: if True use Tensorboard callback
    :param plot_history: if True plots accuracy and loss history per epoch
    :param path:
    :param model_name:
    :return:
    """
    print("\n\n--- Training", type(model).__name__, "---")
    start_time = timeit.default_timer()

    # get the name of the optimizer in the defined model
    opt_config = model.optimizer.get_config()
    if 'name' not in opt_config.keys():
        _name = str(model.optimizer.__class__).split('.')[-1].replace('\'', '').replace('>', '')
        opt_config.update({'name': _name})

    if callbacks:
        # directory to save callbacks
        log_dir = path + model_name + opt_config['name']
        # callbacks: TensorBoard, EarlyStopping, ModelCheckPoint
        # TensorBoard for storing visualizations of the neural net
        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)
        # EarlyStopping to monitor validation loss. If there is any improve after 10 epochs,the training procedure stops
        early_stopping_callback = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=verbose)
        # ModelCheckpoint to monitor validation accuracy. It stores the model with the highest accuracy
        model_checkpoint_callback = ModelCheckpoint('best_model_' + opt_config['name'] + '.h5', monitor='val_accuracy', mode='max',
                                                    verbose=verbose, save_best_only=True)
        if not validation:
            # fit the model
            print("Note: Validation data is not included...Only Tensorboard callback is used!")
            history = model.fit(features, labels, epochs=epochs, batch_size=batch_size, verbose=verbose,
                                callbacks=[tensorboard_callback])  # train the neural network
        else:
            # fit the model
            history = model.fit(features, labels, epochs=epochs, batch_size=batch_size, verbose=verbose,
                                validation_data=(val_data, val_labels),
                                callbacks=[tensorboard_callback, early_stopping_callback, model_checkpoint_callback])
    else:  # train the model without the use of callbacks
        history = model.fit(features, labels, epochs=epochs, batch_size=batch_size, verbose=verbose)

    if plot_history:  # plots the accuracy and loss per epoch
        if validation:
            # summarize history for training and validation accuracy
            plt.plot(history.history['accuracy'])
            plt.plot(history.history['val_accuracy'])
            plt.title('model accuracy')
            plt.ylabel('accuracy')
            plt.xlabel('epoch')
            plt.legend(['train', 'test'], loc='upper left')
            plt.show()
            # summarize history for training and validation loss
            plt.plot(history.history['loss'])
            plt.plot(history.history['val_loss'])
            plt.title('model loss')
            plt.ylabel('loss')
            plt.xlabel('epoch')
            plt.legend(['train', 'test'], loc='upper left')
            plt.show()
        else:
            # print(history.history.keys())
            # summarize history for training accuracy
            plt.plot(history.history['accuracy'])
            plt.title('model accuracy')
            plt.ylabel('accuracy')
            plt.xlabel('epoch')
            plt.legend(['train'], loc='upper left')
            plt.show()
            # summarize history for training loss
            plt.plot(history.history['loss'])
            plt.title('model loss')
            plt.ylabel('loss')
            plt.xlabel('epoch')
            plt.legend(['train'], loc='upper left')
            plt.show()

    stop_time = timeit.default_timer()
    print(type(model).__name__, "training time: ", stop_time - start_time, "seconds\n\n")


def evaluate_neural_network(model, features, labels):
    """
    :param model: neural network model from generate_neural_network()
    :param features: test data
    :param labels: test labels
    :return:
    """
    scores = model.evaluate(features, labels, verbose=0)
    print(model.metrics_names[1], "%.2f%%" % (scores[1] * 100))
    return scores[1] * 100


def test_neural_network(model, test_data, test_labels):
    """
    :param model: neural network model from generate_neural_network()
    :param test_data: validation data
    :param test_labels: validation labels
    :return:
    """
    global average_FNR, average_FPR, average_accuracy
    print(type(model).__name__, "predicting...")
    start_time = timeit.default_timer()
    predicted = model.predict(test_data)
    stop_time = timeit.default_timer()
    # print(predicted)
    # prick the class with higher probability
    confusion = confusion_matrix(test_labels, np.argmax(predicted, axis=1))  # confusion matrix
    print(confusion)
    # confusion matrix metrics
    TP = confusion[1, 1]
    TN = confusion[0, 0]
    FP = confusion[0, 1]
    FN = confusion[1, 0]
    FNR = FN / float(FN + TP) * 100
    FPR = FP / float(FP + TN) * 100
    accuracy = (TP + TN) / float(TP + TN + FP + FN) * 100
    print("FP:", FP, "- FN:", FN, "- TP:", TP, "- TN", TN)
    print("Accuracy:", accuracy, "- FPR:", FPR, "- FNR:", FNR)
    print(type(model).__name__, "prediction time: ", stop_time - start_time, "seconds\n\n")
    average_FNR += FNR
    average_FPR += FPR
    average_accuracy += accuracy


def get_average_metrics(val_runs):
    global average_FNR, average_FPR, average_accuracy
    average_FNR = average_FNR / val_runs
    average_FPR = average_FPR / val_runs
    average_accuracy = average_accuracy / val_runs
    print("Average Accuracy:", average_accuracy, "- Average FPR:", average_FPR, "- Average FNR:", average_FNR)
