"""
This file performs grid search for 'classic' machine learning algorithms.
"""
import set_onehot_encoding as onehot
import os
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier


def grid_RF():
    print("--- Random Forest ---")
    n_estimators = [10, 50, 100, 200]  # number of trees
    criterion = ['gini', 'entropy']  # measurement for the quality of split
    max_features = ['sqrt', 'log2', None]  # Number of features to consider at every split
    min_samples_split = [2, 5, 10]  # Minimum number of samples required to split a node
    min_samples_leaf = [1, 2, 4]  # Minimum number of samples required at each leaf node
    # Create the grid
    param_grid = dict(n_estimators=n_estimators, criterion=criterion, max_features=max_features,
                      min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)

    rf = RandomForestClassifier()  # create the base model to tune
    # Use the grid search to search for best hyperparameters, using 3-fold cross validation
    rf_random = GridSearchCV(estimator=rf, param_grid=param_grid, cv=4, verbose=1,
                             n_jobs=1)  # Fit the grid search model
    grid_result = rf_random.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)  # find the best hyperparameter


def grid_KNN():
    print("--- K Nearest Neighbors ---")
    n_neighbors = [3, 5, 10, 20, 50]  # number of neighbors
    weights = ['uniform', 'distance']  # weight function to use in prediction
    metric = ['euclidean', 'manhattan', 'minkowski']  # distance metric to use

    param_grid = dict(n_neighbors=n_neighbors, weights=weights, metric=metric)

    knn = KNeighborsClassifier()

    knn_grid = GridSearchCV(estimator=knn, param_grid=param_grid, cv=4, n_jobs=-1)
    grid_result = knn_grid.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)


def grid_LR():
    print("--- Logistic Regression ---")
    C = [0.5, 1.0, 1.5, 2.0, 2.5]  # regularization strength
    max_iter = [100, 110, 120, 130, 140]  # maximum number of iterations
    fit_intercept = [True, False]  # add a bias or not to the decision function

    param_grid = dict(max_iter=max_iter, C=C, fit_intercept=fit_intercept)

    lr = LogisticRegression(penalty="l2", solver="lbfgs")

    grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv=4, n_jobs=-1, verbose=1)
    grid_result = grid.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)


def grid_SVM():
    print("--- Support Vector Machines ---")
    C = [0.25, 0.5, 1.0]  # penalty parameter
    kernel = ['linear', 'rbf', 'poly']  # kernel type
    gamma = ['auto', 'scale']  # kernel coefficient
    decision_function_shape = ['ovo', 'ovr']  # one vs rest or one vs one

    param_grid = dict(C=C, kernel=kernel, gamma=gamma, decision_function_shape=decision_function_shape)

    SVM = svm.SVC()

    grid = GridSearchCV(estimator=SVM, param_grid=param_grid, cv=4, n_jobs=1, verbose=1)
    grid_result = grid.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)


def grid_DT():
    print("--- Decision Tree ---")
    criterion = ['gini', 'entropy']  # measurement for the quality of split
    splitter = ['best', 'random']
    max_features = ['sqrt', 'log2', None]  # Number of features to consider at every split
    min_samples_split = [2, 5, 10]  # Minimum number of samples required to split a node
    min_samples_leaf = [1, 2, 4]  # Minimum number of samples required at each leaf node

    param_grid = dict(criterion=criterion, splitter=splitter, max_features=max_features,
                      min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)

    DT = DecisionTreeClassifier()

    rf_random = GridSearchCV(estimator=DT, param_grid=param_grid, cv=4, n_jobs=1)
    grid_result = rf_random.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)


if __name__ == "__main__":
    total_features = 545333  # total unique features
    set_size = 2000  # set site that will be used to create random training and testing set
    malware_ratio = 0.3  # malware ratio in the set size

    onehot.create_list_of_apps()  # function from set_one_encoding.py

    # check if a predefined training sample exists
    if os.path.isfile("training_set_2000.txt") is False:
        print("Creating data-labels...")
        print("Generating TRAINING set...")
        training_set = onehot.generate_set(set_size, malware_ratio)  # generate random training set
        with open("training_set_2000.txt", "w") as file:
            for item in training_set:
                file.write(str(item) + "\n")

    training_set = []  # the list of training set

    with open("training_set_2000.txt", "r") as file:  # read training set file and append applications to list
        for line in file:
            line.strip()
            line = line[:-1]
            training_set.append(line)

    print("Generating TRAINING input...")
    data, labels = onehot.generate_input(training_set, total_features)  # perform one-hot encoding
    print("Grid searching...")

    grid_RF()
    #grid_KNN()
    #grid_LR()
    #grid_SVM()
    #grid_DT()
