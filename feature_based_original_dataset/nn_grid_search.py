import set_onehot_encoding as onehot
import neural_network as NN
from sklearn.model_selection import StratifiedKFold
import numpy as np
import os

def tune_neural_network():
    kfold = StratifiedKFold(n_splits=4, shuffle=True)  # 4-fold cross-validation
    # neural net parameters
    units = [200, 200]  # neurons in each hidden layer
    dropout = 0.2  # dropout rate
    epochs = 5  # epochs
    batch_size = 150  # size in batch
    learn_rate = 0.001 
    #momentum = 0.8  # to work with SGD
    kernel_initializer = 'normal'  # weight init
    bias_initializer = 'normal'  # bias initi
    activation_function = 'relu'

    scores = []

    for train, test in kfold.split(data, labels):  # train on 3 pieces evaluate on 1 (4 total runs)
        model = NN.generate_neural_network(total_features, units, dropout, learn_rate, kernel_initializer,
                                           bias_initializer, activation_function)

        NN.train_neural_network(model, epochs, batch_size, data[train], labels[train])  # train neural network

        score = NN.evaluate_neural_network(model, data[test], labels[test])  # evaluate neural net
        scores.append(score)

    print("Average accuracy: ", np.mean(scores), "Standard Deviation:", np.std(scores))


if __name__ == "__main__":
    total_features = 545333  # total unique features
    set_size = 2000  # set site that will be used to create random training set
    testing_set_size = 2000  # set site that will be used to create random test set
    malware_ratio = 0.3  # malware ratio in the set size

    onehot.create_list_of_apps()  # function from set_one_encoding.py

    # check if a predefined training
    if os.path.isfile("training_set_2000.txt") is False and os.path.isfile("testing_set_2000.txt") is False:
        print("Creating data-labels...")
        print("Generating TRAINING set...")
        training_set = onehot.generate_set(set_size, malware_ratio)  # generate random training set
        with open("training_set_2000.txt", "w") as file:
            for item in training_set:
                file.write(str(item) + "\n")

        print("Generating TESTING set...")
        testing_set = onehot.generate_set(testing_set_size, malware_ratio)  # generate random testing set
        with open("testing_set_2000.txt", "w") as file:
            for item in testing_set:
                file.write(str(item) + "\n")

    training_set = []  # the list of training set
    testing_set = []  # the list of testing set

    with open("training_set_2000.txt", "r") as file:  # read training set file and append applications to list
        for line in file:
            line.strip()  # remove whitespace
            line = line[:-1]  # remove \n
            training_set.append(line)  # add item to list
    with open("testing_set_2000.txt", "r") as file:  # read testing set file and append applications to list
        for line in file:
            line.strip()
            line = line[:-1]
            testing_set.append(line)

    print("Generating TRAINING input...")
    data, labels = onehot.generate_input(training_set, total_features)  # perform one-hot encoding
    print("Generating TESTING input...")
    test_data, test_labels = onehot.generate_input(testing_set, total_features)  # perform one-hot encoding
    tune_neural_network()


"""
# use above code for grid search if you have enough RAM, modifying tune_batch_epochs() method and comment everything above
import set_onehot_encoding as onehot
from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import Sequential
from keras.layers import Dense, Dropout
import os
total_features = 545333  # total unique features
set_size = 2000  # set site that will be used to create random training and testing set
malware_ratio = 0.3  # malware ratio in the set size

onehot.create_list_of_apps()  # function from set_one_encoding.py

if os.path.isfile("training_set.txt") is False and os.path.isfile("testing_set.txt") is False:
    print("Creating data-labels...")
    print("Generating TRAINING set...")
    training_set = onehot.generate_set(set_size, malware_ratio)  # generate random training set
    with open("training_set.txt", "w") as file:
        for item in training_set:
            file.write(str(item) + "\n")

    print("Generating TESTING set...")
    testing_set = onehot.generate_set(testing_set_size, malware_ratio)  # generate random testing set
    with open("testing_set.txt", "w") as file:
        for item in testing_set:
            file.write(str(item) + "\n")

training_set = []

with open("training_set.txt", "r") as file:
    for line in file:
        line.strip()
        line = line[:-1]
        training_set.append(line)

print("Generating TRAINING input...")
data, labels = onehot.generate_input(training_set, total_features)  # perform one-hot encoding


def create_model():
    model = Sequential()

    model.add(Dense(units=200, activation="relu", input_dim=total_features))
    model.add(Dropout(0.5))  # add dropout

    model.add(Dense(units=200, activation="relu"))
    model.add(Dropout(0.5))

    model.add(Dense(2, activation="softmax"))  # output layer, with softmax activation function and 2 neurons

    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer='adam',
                  metrics=["accuracy"])
    # loss sparse categorical, Adam optimizer
    model.summary()
    return model


def tune_batch_epochs():

    model = KerasClassifier(build_fn=create_model, verbose=1)

    epochs = [5, 10, 15, 20]
    batch_size = [50, 100, 128, 200]
    optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
    learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]
    # momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]  # to work with SGD
    kernel_initializer = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']  # weight init
    param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer, learn_rate=learn_rate, kernel_initializer=kernel_initializer)

    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)
    grid_result = grid.fit(data, labels)

    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_))
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print(mean, stdev, "with",  param))


tune_batch_epochs()
"""
