import set_onehot_encoding as onehot
import feature_based_original_dataset.models as models
import neural_network as NN
import numpy as np
import os


def create_sets():

    if os.path.isfile("training_set_8500.txt") is False:
        set_size = 8500
        malware_ratio = 0.3
        print("Creating data-labels...")
        print("Generating TESTING set...")
        training_set = onehot.generate_set(set_size, malware_ratio)  # generate random testing set
        with open("training_set_1500.txt", "w") as file:
            for item in training_set:
                file.write(str(item) + "\n")

    if os.path.isfile("testing_set_8500.txt") is False:
        set_size = 8500
        malware_ratio = 0.3
        print("Creating data-labels...")
        print("Generating TESTING set...")
        testing_set = onehot.generate_set(set_size, malware_ratio)  # generate random testing set
        with open("testing_set_1500.txt", "w") as file:
            for item in testing_set:
                file.write(str(item) + "\n")

    training_set = []
    testing_set = []

    with open("training_set_8500.txt", "r") as file:  # read training set file and append applications to list
        for line in file:
            line.strip()  # remove whitespace
            line = line[:-1]  # remove \n
            training_set.append(line)  # add item to list
    with open("testing_set_8500.txt", "r") as file:  # read testing set file and append applications to list
        for line in file:
            line.strip()
            line = line[:-1]
            testing_set.append(line)
    print("Generating TRAINING input...")
    data, labels = onehot.generate_input(training_set, total_features)  # perform one-hot encoding
    print("Generating TESTING input...")
    test_data, test_labels = onehot.generate_input(testing_set, total_features)  # perform one-hot encoding
    return data, labels, test_data, test_labels


def train_models():
    data, labels, test_data, test_labels = create_sets()

    model = NB.train_multi_naive_bayes_classifier(data,labels, save=False)
    NB.test_multi_naive_bayes_classifier(model, test_data, test_labels)

    model = DT.train_decision_tree_classifier(data, labels, save=False)
    DT.test_decision_tree_classifier(model, test_data, test_labels)

    model = RF.train_random_forest_classifier(data, labels, save=False)
    RF.test_random_forest_classifier(model, test_data, test_labels)

    model = KNN.train_knn_classifier(data, labels, save=False)
    KNN.test_knn_classifier(model, test_data, test_labels)

    model = LR.train_logistic_regression_classifier(data, labels, save=False)
    LR.test_logistic_regression_classifier(model, test_data, test_labels)

    model = SVM.train_svm_classifier(data, labels, save=False)
    SVM.test_svm_classifier(model, test_data, test_labels)

    # init the neural net
    model = NN.generate_neural_network(total_features, units, dropout, learn_rate, kernel_initializer,
                                       bias_initializer, activation_function)
    """
    train the neural network with the given model, epochs, batch size, train data-labels.
    Specify verbosity level, validation data, callbacks and plots if needed.
    Default parameters:
    verbose=0, validation=False, val_data=None, val_labels=None, callbacks=False, plot_history=False
    example:
    NN.train_neural_network(model, epochs, batch_size, data, labels, verbose=0,
                            validation=True, val_data=test_data, val_labels=test_labels,
                            callbacks=True, plot_history=True)
    This is the main training stage and thus we want to save the best models at the right times. This is done
    setting the callback to True. Keras will seek for the minimum validation loss and it saves the model with
    the highest validation accuracy.
    """
    NN.train_neural_network(model, epochs, batch_size, data, labels, verbose=2,
                            validation=True, val_data=test_data, val_labels=test_labels,
                            callbacks=True)
    NN.test_neural_network(model, test_data, test_labels)


if __name__ == "__main__":
    total_features = 3880  # total unique features
    set_size = 8500  # set site that will be used to create random training set
    testing_set_size = 8500  # set site that will be used to create random test set
    malware_ratio = 0.3  # malware ratio in the set size

    print("Creating data-labels...")
    onehot.create_list_of_apps()  # function from set_one_encoding.py

    # initialize sklearn models
    NB = models.MultinomialNaiveBayes()
    DT = models.DecisionTree()
    RF = models.RandomForest()
    KNN = models.KNearestNeighbors()
    LR = models.LogRegression()
    SVM = models.SupportVectorMachine()

    val_runs = 8
    # neural net parameters
    units = [200, 200]
    dropout = 0.2
    epochs = 20
    batch_size = 150
    learn_rate = 0.001
    # momentum = 0.0  # to work with SGD
    kernel_initializer = 'glorot_uniform'
    bias_initializer = 'zeros'
    activation_function = 'relu'

    train_models()
