import set_onehot_encoding as onehot
import models
import neural_network as NN
import numpy as np

def create_random_sets():
    print("Generating TRAINING set...")
    training_set = onehot.generate_set(set_size, malware_ratio)  # generate random training set
    print("Generating TRAINING input...")
    data, labels = onehot.generate_input(training_set, total_features)  # perform one-hot encoding
    print("Generating TESTING set...")
    testing_set = onehot.generate_set(testing_set_size, malware_ratio)  # generate random testing set
    print("Generating TESTING input...")
    test_data, test_labels = onehot.generate_input(testing_set, total_features)  # perform one-hot encoding
    return data, labels, test_data, test_labels  # return train data - labels and test data - labels


def random_sub_sampling(runs):

    score_nn = []
    score_rf = []
    score_lr = []
    score_dt = []
    score_svm = []
    score_knn = []

    for i in range(runs):

        data, labels, test_data, test_labels = create_random_sets()  # choose random training and testing sets

        """# init neural net
        model = NN.generate_neural_network(total_features, units, dropout, learn_rate, kernel_initializer,
                                           bias_initializer, activation_function)
        '''
        this is not the actual training procedure and we don't want to save the models. To save models and implement
        the early stopping technique refer to train_models.py
        The goal of this operation is only to determine the behavior of models to random training sets and random
        testing sets!
        So, only train and evaluate models.
        '''
        NN.train_neural_network(model, epochs, batch_size, data, labels, verbose=2)
        score = NN.evaluate_neural_network(model, test_data, test_labels)
        score_nn.append(score)"""

        #model = DT.train_decision_tree_classifier(data, labels)  # train Decision Tree Classifier
        #score_dt = DT.evaluate_decision_tree_classifier(model, test_data, test_labels)

        #model = RF.train_random_forest_classifier(data, labels)  # train Random Forest
        #score_rf = RF.evaluate_random_forest_classifier(model, test_data, test_labels)

        model = KNN.train_knn_classifier(data, labels)  # train k-Nearest Neighbors Classifier
        score_knn = KNN.evaluate_knn_classifier(model, test_data, test_labels)

        #model = LR.train_logistic_regression_classifier(data, labels)  # train logistic Regression
        #score_lr = LR.evaluate_logistic_regression_classifier(model, test_data, test_labels)

        #model = SVM.train_svm_classifier(data, labels)  # train Support Vector Machines
        #score_svm = SVM.evaluate_svm_classifier(model, test_data, test_labels)

    #print("NN Average accuracy: ", np.mean(score_nn), "Standard Deviation:", np.std(score_nn))
    #print("DT Average accuracy: ", np.mean(score_dt), "Standard Deviation:", np.std(score_dt))
    #print("RF Average accuracy: ", np.mean(score_rf), "Standard Deviation:", np.std(score_rf))
    print("kNN Average accuracy: ", np.mean(score_knn), "Standard Deviation:", np.std(score_knn))
    #print("LR Average accuracy: ", np.mean(score_lr), "Standard Deviation:", np.std(score_lr))
    #print("SVM Average accuracy: ", np.mean(score_svm), "Standard Deviation:", np.std(score_svm))



if __name__ == "__main__":
    total_features = 3880  # total unique features
    set_size = 8500  # set site that will be used to create random training set
    testing_set_size = 8500  # set site that will be used to create random test set
    malware_ratio = 0.3  # malware ratio in the set size

    print("Creating data-labels...")
    onehot.create_list_of_apps()  # function from set_one_encoding.py

    DT = models.DecisionTree()
    RF = models.RandomForest()
    KNN = models.KNearestNeighbors()
    LR = models.LogRegression()
    SVM = models.SupportVectorMachine()

    val_runs = 8  # number of times to train and test a model

    # neural net parameters
    units = [200, 200]  # number of neurons in each layer (2 hidden layers)
    dropout = 0.2  # dropout rate
    epochs = 18  # epochs per iteration
    batch_size = 150  # batch size
    learn_rate = 0.001  # learning rate of the specified optimizer
    kernel_initializer = 'glorot_uniform'  # weight initialization
    bias_initializer = 'zeros'  # bias initialization
    activation_function = 'relu'  # activation function in hidden layers (We use Softmax in the output layer)

    random_sub_sampling(val_runs)
