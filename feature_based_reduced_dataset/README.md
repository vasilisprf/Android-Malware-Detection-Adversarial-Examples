The process is similar to the feature_based_original_dataset. In the reduced feature space we introduce an adversarial sample detector.

### 1) Feature Reduction

The huge variety of features in the dataset leads to a high dimensionality and as a result the data become sparse. The sparsity leads to vast computation cost making the whole dataset unavailable for models to process it in a single run. Moreover, this means that in the dataset present features which have a minimal importance for the final decision of a classifier. In literature there exists algorithmic approaches to feature elimination and dimensionality reduction, but we stick with a manual/“regular” reduction due the high computation cost. An adversary can change a network address without much effort. With the elimination of this class, the features are decreased to 234,845, less than half of the original feature space. Furthermore, each Android application has activities, which in essence is the user interface. The user interface does not constitute a significant matter for the classification task as the names can be random and changed. Activities are contained in Components class with a total of 185,729 features. Therefore, the feature space is reduced to 49,116. Moreover, some features are found to a large extent to both benign and malware applications and as such, these features are not important for classification. These are only 6. Consequently, the feature space is reduced to 49,110 from the original 545,333 features. 

```
python3 eliminate_low_high_support_features.py
```
```
python3 eliminate_features.py
```

As for the applications themselves, we observed that there exist duplicates, 37,077 in particular. The duplicates, we are referring to, are applications that have the same features. This does not mean that the applications are exactly the same, but that they have similar functionality. Specifically for malicious applications, the duplication may mean that these applications are variants of the original malware. With duplicate applications, the amount of applications can be significantly reduced and thus, the algorithms can fit more applications into one run. Finally, we were able to reduce the dataset from to 129,013 (5, 560 malicious) to 91,936 applications (2591 malicious) and from 545,333 unique features to 3880. To find and remove the duplicate applications, we adjusted [this](http://www.davespace.co.uk/python/remove-duplicates.html) python script.

### 2) Training, Evaluating, Crafting adversarial examples

Followe README file in feature_based_original_dataset folder.

### 3) Defense by [detecting adversarial examples](https://arxiv.org/pdf/1702.06280.pdf)

Another defensive approach, similar to the adversarial training, is to detect adversarial examples prior the feeding of samples into the main classification model. Our approach produces an external classifier, which is able to classify samples as legitimate or adversarial. 

Steps for our model:
1)	Train a classifier F on the original data D={X,0}, labeling all the training set as 0.
2)	Craft adversarial examples A for F using the JSMA crafting method.
3)	Train a new model F' on an augmented dataset X⋃A, labeling each adversarial example as 1.
4)	Before feeding a new sample to the classifier F, the sample passes through the detector for classification. If the sample is recognized as malicious the process stops. 

We evaluate the classification accuracy of the newly generated classifier in the reduced feature space as an attempt to defend against the adversarial examples that deceive the model trained with the Adam optimizer. The original performance of the learned model on the reduced feature space is 98.42% (1.58% FNR). With the JSMA variant the model is completely destroyed as the attack makes the model unable to recognize any malicious application, increasing the FNR to 100%. Similar to adversarial training, there is any specific methodology to follow for the mixing of adversarial examples with a specific legitimate samples ratio.  

```
python3 detector.py
```

We begin training a model only on malware space without the presence of any benign applications. Note that we use every piece of malware sample to craft adversarial examples and to train the adversarial detector. We expect the detector to be highly efficient in distinguish adversarial examples from legitimate samples, but without the ability to accurate classify the samples when mixed with benign and malware samples. Indeed, our detector gets a training accuracy of 99.21% at epoch 15. This means that it may be able to classify adversarial samples with high probability. However, the detector is only trained on the malware space. To get a good estimation, we mix benign and malicious applications, we craft adversarial examples for the original model and we evaluate the detector. As expected, the classification performance is not as high, achieving 83.7% with 16.3% FPR without the presence of an adversary. When adversarial examples crafted, the accuracy slightly decreased to 83.5% with 23.14% FPR and only 1% FNR. This means that only a few adversarial examples bypassed the detector. 

The FPR in our first evaluation in the detector trained only with malicious applications is quite high. Therefore, we hope that mixing the benign with malicious applications and crafting adversarial examples only for malware will increase the overall accuracy. We get a sample totaling 2000 applications, 600 of them malicious. This means that the detector is trained on 2000 legitimate samples and 600 adversarial. We re-implement the ModelCheckpoint callback to store only the best model in terms of accuracy. The model stored at epoch 24 where the training accuracy is 98.58%. Next, we craft adversarial examples for the original model in a testing set totaling 8500 applications, where almost malicious applications is present (totaling 2,550 in the testing set) and we evaluate the performance of the trained detector. Surprisingly, the performance is extremely high with and without the presence of adversarial examples. In legitimate applications the model achieves 99.72% accuracy (with 0.28% FPR) and with the presence of an adversary, the model achieves 98.1% accuracy with 5.46% FNR and 0.39% FPR.

Training on a sample of the dataset results in extremely high performance: only 5% of the adversarial examples can bypass our security mechanism. However, this means that the 5% of the adversarial malware will be mistakenly identified as benign in the main detector. We also evaluate whether the training on a larger samples, in the presence of almost every malicious application, can create a more efficient model. This can be described as an incremental procedure of the detector. As mentioned, incremental learning may give a false sense of performance, since the malicious applications that are in the wild are by no means covered. We train the detector with a set of 8500 applications, 2550 of them malicious. The detector achieves 99.27% accuracy in the training stage (with 0.45% FPR). In the testing stage it achieves an extremely high performance of 99.98% (only 2 legitimate applications are recognized as adversarial) and with the presence of adversarial examples achieves 99.54% accuracy with only 1.49% FNR. Therefore, the efficiency of the detector is dramatically improved. The higher the performance of the adversarial detector, the better for our main model, as the detector can eliminate adversarial examples. 

We showed that training a second classifier that distinguishes original samples from adversarial ones can be used as a defensive mechanism. Only the applications that map as legitimate is passed through the main detector for classification. However, since an intelligent attacker can deceive the main classifier, then it will be easy to deceive the adversarial detector. If the adversary is aware of the external classifier, its goal is to produce adversarial examples that are classified as legitimate. The procedure is similar to the adversarial examples crafting for the main model. The purpose is to classify an adversarial sample that correctly classified as adversarial to a benign one. As such, we craft adversarial examples for the adversarial detector in a test set of 2000 applications. Its original performance is 100% without the presence of an adversary and 99.5% (1.67% FNR) on adversarial examples produced for the main classifier. As expected, when crafting adversarial examples for the detector, the classifier is almost completely fooled. Its accuracy is 70.2%, with 99.33% FNR and 9 average perturbations in the feature space. 
